#!/usr/bin/python3
#
# Retrieve all raw bug reports across multiple iterations of CS2103/T and CS2113,
# which are generated by students for Practical Exam (and the dry-run)
#
# Author: Justin
# Created Date: 2021-04-04
# Modified By: Kenneth 
# Modified Date: 2024-04-19
# Note:
# - Uses iP forks to retrieve list of students from each iteration.
#   Does a quick and dirty job as of date specified (AY2021 Sem 2 CS2103)
# - Current limits on GitHub API is 60 (5000) per hour for unauthenticated (authenticated) users
#   Rate limits: https://docs.github.com/en/rest/overview/resources-in-the-rest-api#rate-limiting
#   Getting a token: https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token
# - Requires 'beautifulsoup4 and 'requests', available via 'pip'
# - Terminate the issue retrieval using SIGINT / 'Ctrl-C'

import os
import bs4
import requests
import time
import json

# Create or regenerate Github Token
# Allow the following access:
#      repo:status , repo_development , public_repo , repo:invite , read:pacakages , read:public_key
#      gist , read:user , read:discussion , read:enterprise, read:audit_log , read:project , read:gpg_key

# GITHUB_TOKEN = "Bearer <Insert key here> " # delete variable or set blank (default) to disable authentication


LIST_IP_FORKS = [
    # - AY2324S1 (CS2103/T)
    "https://github.com/nus-cs2103-AY2324S1/ip/network/members",
    # - AY2223S2 (CS2103/T)
    "https://github.com/nus-cs2103-AY2223S2/ip/network/members",
    # - AY2223S1 (CS2103/T)
    "https://github.com/nus-cs2103-AY2223S1/ip/network/members",
    # - AY2122S2 (CS2103/T)
    "https://github.com/nus-cs2103-AY2122S2/ip/network/members",
    # - AY2122S1 (CS2103/T)
    "https://github.com/nus-cs2103-AY2122S1/ip/network/members",
    # - AY2021S2 (CS2103/T)
    "https://github.com/nus-cs2103-AY2021S2/ip/network/members",
    # - AY2021S1 (CS2103)
    "https://github.com/nus-cs2103-AY2021S1/ip/network/members",
]

#### Modify below at your own risk ####

# Set up endpoint variables
ENDPT_FMT = "https://api.github.com/repos/{}/{}/issues"
HEADERS = {}
if "GITHUB_TOKEN" in globals() and GITHUB_TOKEN:
    # PARAMS["access_token"] = GITHUB_TOKEN # pass token for authentication
    HEADERS["Authorization"] = GITHUB_TOKEN  
    GITHUB_API_RATE_LIMIT = 5000 # per hour, being a responsible internet citizen
else:
    GITHUB_API_RATE_LIMIT = 60

# PE and PE-D repository
REPO_NAME_PE = "pe"
REPO_NAME_PED = "ped"

# Buffer to hold issues information, and pipe to file
# Used for no real good reason, C++ habits I guess
ISSUES = [] # format: [(severityLabel, typeLabel, username, reponame, reportTitle, reportBody), ...]

def tag_predicate(s):
    """ format: ^/[a-zA-Z0-9_-]+$
    
    Strings 'CS2113' and 'CS2103' are intentionally excluded:
    1. Avoid pulling from temporary throwaway accounts
    2. Avoid accidental retrieval from upstream organization
    """
    return s.startswith("/") \
           and s.find("/", 1) == -1 \
           and s.find("?") == -1 \
           and s.lower().find("cs2113") == -1 \
           and s.lower().find("cs2103") == -1

# Access cached user list
print("\n=================================")
try:
    with open("user_list.txt", "r") as f:
        users = sorted(
            [user.strip() for user in f.read().split("\n") if user != ""],
            key=lambda s: s.lower()
        ) # not necessary, but for posterity
    print("Pulled user list from cache.")
    
except:
    print("Retrieving user list...")
    users = set() # ignore duplicates
    for fork_list in LIST_IP_FORKS:
        r = requests.get(fork_list)
        soup = bs4.BeautifulSoup(r.content, "html.parser")

        # Get all usernames in module
        # ... give it a shot yourself using Ctrl-Shift-I in Chrome
        for tag in soup.findAll(
                "a", **{
                    "href": tag_predicate,
                    "class": "d-inline-block",
                }):
            user = tag.get("href")[1:] # remove /
            users.add(user)

    users = sorted(users, key=lambda s: s.lower())
    with open("user_list.txt", "w") as f:
        for user in users:
            f.write(user + "\n")
    print("User list retrieved, cached in './user_list.txt'")

def get_issues(user, repo):
    r = requests.get(ENDPT_FMT.format(user, repo), headers=HEADERS)
    d = json.loads(r.content.decode())
    if r.status_code != 200:
        if r.status_code == 403 and "API rate limit exceeded" in d["message"]:
            return False # API rate limited
        return True # No permission to access repo
            
    target = ISSUES
    issues = d
    for issue in issues:
        title = issue["title"]
        
        # Retrieve and clean body
        body = issue["body"]
        metadata_idx = body.rfind("<!--session") # Added by CATcher...?
        if metadata_idx != -1:
            body = body[:metadata_idx].strip()
        body = body.replace("\n", " ") \
                   .replace("\r", " ") \
                   .replace("\t", " ") # oneline
        
        # Retrieve and clean labels
        labels = [label["name"] for label in issue["labels"]]
        ignore_issue = False
        for label in labels:
            if "invalid" in label.lower(): # technically should not occur since author's repo
                ignore_issue = True
                break
        if ignore_issue:
            continue
        labels = [label for label in labels if (label.startswith("type") or label.startswith("severity"))]
        if len(labels) != 2:
            continue # author did funny things with the issue... better not touch
        labels.sort() # severity first, then type

        # Okay
        target.append((labels[0], labels[1], user, repo, title, body))

    return True # Success

def save_issues():
    """ Writes incrementally to avoid overloading memory.
    
    - Tab delimited format to allow Excel to easily parse information.
    - UTF-8 because some jokers decided using emojis in their bug report
      is a good idea (it is :>)
    """
    with open("issues_list.txt", "a", encoding="utf-8") as f:
        for issue in ISSUES:
            f.write("{}\t{}\t{}\t{}\t{}\t{}\n".format(*issue))
    ISSUES.clear()

def get_remaining_calls():
    r = requests.get("https://api.github.com/rate_limit", headers=HEADERS)
    d = json.loads(r.content.decode())
    print(d)
    left = d["resources"]["core"]["remaining"]
    return left

# For user indication as to whether process will be rate-limited
total_calls_required = len(users) * 2
remaining_calls = get_remaining_calls()
limit_calls = False
print("\n=================================")
print("Total calls required:", total_calls_required)
print("Remaining calls:", remaining_calls)
if remaining_calls < total_calls_required:
    print("API calls are rate-limited to prevent overflow.")
    limit_calls = True
else:
    print("API rate limit will not be applied.")

print("\n=================================")
try:
    for i, user in enumerate(users, start=1):
        print("Currently {: >4d} / {: >4d} ({})".format(i, len(users), user))

        # PED repository
        while True:
            if limit_calls:
                time.sleep(3600/GITHUB_API_RATE_LIMIT)
            if get_issues(user, REPO_NAME_PED):
                break
                
        # PE repository
        while True:
            if limit_calls:
                time.sleep(3600/GITHUB_API_RATE_LIMIT)
            if get_issues(user, REPO_NAME_PE):
                break
            
        save_issues()

except KeyboardInterrupt:
    print("\nUser terminated operation.")
    print("{} user retrieved, data stored in: './issues_list.txt'".format(i))
else:
    print("\nRetrieval completed, data stored in: './issues_list.txt'")
print("\n=================================")
